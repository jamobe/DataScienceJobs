{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4, time\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "parent_folder, current_folder = os.path.split(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape all links to job postings containing the word 'Data'\n",
    "First searching for all job postings using the word 'Data' on www.cwjobs.de.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jobtype(soup):\n",
    "    jobtype=[x.text for x in soup.find_all('li',{'class':'job-type'})]\n",
    "    return jobtype\n",
    "\n",
    "def extract_full_desc(soup):\n",
    "    text=[x.text for x in soup.find_all('div',{'class':'job-description'})]\n",
    "    return text\n",
    "\n",
    "def extract_links(soup):\n",
    "    links =[]\n",
    "    for div in soup.find_all(name='div', attrs={'class':'job-title'}):\n",
    "        for a in div.find_all('a'):\n",
    "            links.append(a['href'])\n",
    "    return links\n",
    "\n",
    "def extract_company_from_result(soup): \n",
    "    company = []\n",
    "    for div in soup.find_all(name=\"li\", attrs={\"class\":\"company\"}):\n",
    "        company.append(div.text.strip())\n",
    "        \n",
    "    return(company)\n",
    "\n",
    "def extract_date_from_result(soup): \n",
    "    date = []\n",
    "    for div in soup.find_all('li',{'class':'date-posted'}):\n",
    "        date.append(div.text.strip())\n",
    "   \n",
    "    return(date)\n",
    "\n",
    "def extract_location_from_result(soup): \n",
    "    location = []\n",
    "    for div in soup.find_all('li', {'class':'location'}):\n",
    "        for a in div.find('a'):\n",
    "            location.append(a)\n",
    "   \n",
    "    return(location)\n",
    "\n",
    "def extract_salary_from_result(soup): \n",
    "    salaries = []\n",
    "    for div in soup.find_all(name=\"li\", attrs={\"class\":\"salary\"}):\n",
    "        try:\n",
    "            salaries.append(div.text)\n",
    "        except:\n",
    "            salaries.append(\"Nothing_found\")\n",
    "    return(salaries)\n",
    "\n",
    "def extract_job_title_from_result(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"job-title\"}):\n",
    "        for a in div.find_all(name=\"h2\"):\n",
    "            jobs.append(a)\n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping code:\n",
    "\n",
    "#decide what search term to use for finding jobs\n",
    "searchTerm=\"data\"\n",
    "\n",
    "#create empty data frame with column headers\n",
    "ads=pd.DataFrame(columns=['company','title','salary','location','date','full_description','jobtype','url'])\n",
    "\n",
    "# loop for scraping\n",
    "\n",
    "for i in range(0, 123):\n",
    "    company = []\n",
    "    job_title = []\n",
    "    description = []\n",
    "    salary = []\n",
    "    location = []\n",
    "    date = []\n",
    "    full_description = []\n",
    "    text_list = []\n",
    "    type_list = []\n",
    "    \n",
    "    time.sleep(1) #ensuring at least 1 second between page grabs\n",
    "    url = 'https://www.cwjobs.co.uk/jobs/'+searchTerm+'?s=header&page='+str(i)\n",
    "    res = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(res.content)\n",
    "    df = pd.DataFrame(columns=['company','title','salary','location','date','full_description','jobtype','url'])\n",
    "    df['company'] = extract_company_from_result(soup)\n",
    "    df['title'] = extract_job_title_from_result(soup)\n",
    "    df['salary'] = extract_salary_from_result(soup)\n",
    "    df['location'] = extract_location_from_result(soup)\n",
    "    df['date'] = extract_date_from_result(soup)\n",
    "    \n",
    "    sub_urls=extract_links(soup)\n",
    "    for j in sub_urls:\n",
    "        res_sub = requests.get(j)\n",
    "        soup_sub = bs4.BeautifulSoup(res_sub.content)\n",
    "        desc = extract_full_desc(soup_sub)\n",
    "        jobtype = extract_jobtype(soup_sub)\n",
    "        text_list.append(desc)\n",
    "        type_list.append(jobtype)\n",
    "        \n",
    "    df['full_description'] = text_list\n",
    "    df['jobtype'] = type_list\n",
    "    df['url'] = sub_urls\n",
    "\n",
    "    ads = ads.append(df, ignore_index=True)\n",
    "\n",
    "today = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "ads.to_csv(parent_folder+'/data/cwjobs_'+today+'.csv', index=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish how many ads had no salary\n",
    "ads['salary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
