{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import model_encodings_jm as e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data from Database\n",
    "1. Only take the data with English Job descriptions (for BOG and TFIDF it is necessary that the job descriptions have all the same language)\n",
    "2. Drop Rows that contain NaNs in the important Columns (the columns that we will use as features)\n",
    "3. Only considers the yearly salary (monthly, weekly salary will mess up the results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jamoth/DSR/DataScienceJobs/data/SQL_access.pkl','rb') as file:\n",
    "       PASSWORD = pickle.load(file)\n",
    "engine = create_engine('postgresql://postgres:'+PASSWORD+'@dsj-1.c9mo6xd9bf9d.us-west-2.rds.amazonaws.com:5432/')\n",
    "df = pd.read_sql(\"select * from all_data where language like'en'\", engine)\n",
    "\n",
    "df1 = df.dropna(subset = ['salary_average_euros','region','country','train_test_label','company','description'], axis=0)\n",
    "df1 = df1.loc[df1.salary_type == 'yearly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data in Train, Validation and Test sets\n",
    "1. Reset the index of the dataframe (as we dropped some rows in our previous steps)\n",
    "2. Split the data based on the assignment in the 'Train_test_label' column from the database.\n",
    "3. Split the Train Set into Train and Validation set\n",
    "4. Track the index for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "#first split the train from the test as denoted in the database\n",
    "df_train = df1.loc[df1['train_test_label']=='train']\n",
    "x_test = df1.loc[df1['train_test_label']=='test']\n",
    "df_train_y = df_train['salary_average_euros']\n",
    "y_test = x_test['salary_average_euros']\n",
    "\n",
    "# then split the train data into train and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_train, df_train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_index = x_train.index\n",
    "val_index = x_val.index\n",
    "test_index = x_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding\n",
    "1. Define which columns should be One-Hot-Encoded: Company, Country and Region\n",
    "2. Fit the One-Hot-Encoding model only on the Train set\n",
    "3. Transform the Train, Validation and Test data using the One-Hot-Encoding model\n",
    "\n",
    "Result: Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_ohe_encode = ['company','country','region']\n",
    "train_enc = x_train[columns_to_ohe_encode]\n",
    "val_enc = x_val[columns_to_ohe_encode]\n",
    "test_enc = x_test[columns_to_ohe_encode]\n",
    "\n",
    "# only train encoding on train data\n",
    "enc = preprocessing.OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "enc.fit(train_enc)\n",
    "\n",
    "# get the names of the OHE features\n",
    "col_headings = enc.get_feature_names(columns_to_ohe_encode)\n",
    "\n",
    "# create encoding\n",
    "OHE_train = enc.transform(train_enc).toarray()\n",
    "OHE_val = enc.transform(val_enc).toarray()\n",
    "OHE_test= enc.transform(test_enc).toarray()\n",
    "\n",
    "# allocate columns names\n",
    "#OHE_train.columns = col_headings\n",
    "#OHE_val.columns  = col_headings\n",
    "#OHE_test.columns  = col_headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BOG) Encoding\n",
    "1. Fit the BOG model with the Train set\n",
    "2. Transform Train, Validation and Test set using the BOG model\n",
    "\n",
    "Result: Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected only English job descriptions...\n",
      "\n",
      "Performed some basic text cleaning...\n",
      "\n",
      "Trained Bag-Of-Words model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BOG_model = e.encode_BOG(x_train,min_df = 3)\n",
    "\n",
    "BOG_train= BOG_model.transform(x_train['description']).toarray()\n",
    "BOG_val= BOG_model.transform(x_val['description']).toarray()\n",
    "BOG_test= BOG_model.transform(x_test['description']).toarray()\n",
    "\n",
    "#feature_names_bog = BOG_model.get_feature_names()\n",
    "#BOG_train.columns = feature_names_bog\n",
    "#BOG_val.columns = feature_names_bog\n",
    "#BOG_test.columns = feature_names_bog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency â€“ Inverse Document Frequency (TF-IDF) Encoding\n",
    "1. Fit the TF-IDF model with the Train set\n",
    "2. Transform Train, Validation and Test set using the TF-IDF model\n",
    "\n",
    "Result: Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected only English job descriptions...\n",
      "\n",
      "Performed some basic text cleaning...\n",
      "\n",
      "Trained TF-IDF model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TFIDF_model= e.encode_TFIDF(x_train,min_df = 3)\n",
    "TFIDF_train= TFIDF_model.transform(x_train['description']).toarray()\n",
    "TFIDF_val= TFIDF_model.transform(x_val['description']).toarray()\n",
    "TFIDF_test= TFIDF_model.transform(x_test['description']).toarray()\n",
    "\n",
    "#feature_names_tfidf = TFIDF_model.get_feature_names()\n",
    "#TFIDF_train.columns = feature_names_tfidf\n",
    "#TFIDF_val.columns = feature_names_tfidf\n",
    "#TFIDF_test.columns = feature_names_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of Technical Terms from Job description\n",
    "1. Load dictionary with technical terms from Pickle file\n",
    "2. Select the categories, which should be included\n",
    "3. Create a list with all technical terms\n",
    "4. Extract a list of technical terms that occur in the job description (for Train, Validation and Test set)\n",
    "5. Fit a Multilabelbinarizer model using the Train set\n",
    "6. Transform Train, Validation and Test set using the Multilabelbinarizer model\n",
    "\n",
    "Result: Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_dict =  pd.read_pickle('Pickles/broad_tech_dictionary.pickle')\n",
    "categories_to_include = ['front_end-technologies', 'databases', 'software-infrastructure-devops','data-science','software_architecture', 'web_design','tools','cyber_security','cloud_computing','back_end-technologies', 'mobile']\n",
    "\n",
    "tech_list=[]\n",
    "\n",
    "for i in categories_to_include:\n",
    "    for j in range(len(tech_dict[i])):\n",
    "        tech_list.append(tech_dict[i][j])\n",
    "important_terms = list(set([x.lower() for x in tech_list]))\n",
    "\n",
    "tech_terms_train = x_train['description'].apply(e.tech_process,args=(important_terms,))\n",
    "tech_terms_val = x_val['description'].apply(e.tech_process,args=(important_terms,))\n",
    "tech_terms_test = x_test['description'].apply(e.tech_process,args=(important_terms,))\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = important_terms)\n",
    "mlb.fit(tech_terms_train)\n",
    "TECH_train = mlb.transform(tech_terms_train)\n",
    "TECH_val = mlb.transform(tech_terms_val)\n",
    "TECH_test = mlb.transform(tech_terms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of each Encoding-Matrices:\n",
    "1. OHE matrix -> encoding Company, Country, Region\n",
    "2. TFIDF matrix -> Encoding job descriptions\n",
    "3. TECH matrix -> Encoding technical terms from the job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4104, 1553)\n",
      "(4104, 7568)\n",
      "(4104, 1433)\n"
     ]
    }
   ],
   "source": [
    "print(OHE_train.shape)\n",
    "print(TFIDF_train.shape)\n",
    "print(TECH_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Encoding-Matrices have the same number of rows, you can horizontally stack them together, to create the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = np.hstack((OHE_train, TFIDF_train, TECH_train))\n",
    "X_Val = np.hstack((OHE_val, TFIDF_val, TECH_val))\n",
    "X_Test = np.hstack((OHE_test, TFIDF_test, TECH_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4104, 10554)\n",
      "(4104,)\n"
     ]
    }
   ],
   "source": [
    "print(X_Train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
